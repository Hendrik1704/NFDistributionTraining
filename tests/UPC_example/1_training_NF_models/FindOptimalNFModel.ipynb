{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ebf495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import time\n",
    "import pickle\n",
    "from scipy.stats import entropy\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "# Get the path three levels up and add it to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..')))\n",
    "\n",
    "from src.realnvp.utils import load\n",
    "\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"Memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a463077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "def load_training_dataset(path_comparison_data, supervised=True):\n",
    "    try:\n",
    "        with open(path_comparison_data, 'rb') as pf:\n",
    "            data_raw = pickle.load(pf)\n",
    "        if supervised:\n",
    "            return np.array(data_raw[:, :-1])\n",
    "        else:\n",
    "            return np.array(data_raw)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {path_comparison_data} not found. Please check the path.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca695a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search parameters\n",
    "batch_sizes = [500, 1000, 2000, 5000]\n",
    "layer_counts = [4, 6, 8, 10, 12]\n",
    "learning_rates = [1e-3, 5e-4, 1e-4, 5e-5]\n",
    "\n",
    "combinations = list(itertools.product(batch_sizes, layer_counts, learning_rates))\n",
    "\n",
    "base_paths = [\n",
    "    \"./eP/\",\n",
    "]\n",
    "\n",
    "corresponding_model_dir_names = [\n",
    "    \"models_eP/\",\n",
    "]\n",
    "\n",
    "filename_prefixes = [\n",
    "    \"eP_nf_bs\",\n",
    "]\n",
    "\n",
    "supervised = [True]\n",
    "\n",
    "model_names = []\n",
    "for base_path, model_dir, filename_prefix in zip(base_paths, corresponding_model_dir_names, filename_prefixes):\n",
    "    setup_model_names = []\n",
    "    for batch_size, layers, lr in combinations:\n",
    "        model_name = f\"{base_path}{model_dir}{filename_prefix}{batch_size}_L{layers}_lr{lr:.0e}.pkl\"\n",
    "        setup_model_names.append(model_name)\n",
    "    model_names.append(setup_model_names)\n",
    "\n",
    "# Check results\n",
    "print(f\"{len(combinations)} combinations per setup.\")\n",
    "print(f\"{len(model_names)} total setups.\")\n",
    "for i, (setup, names) in enumerate(zip(base_paths, model_names)):\n",
    "    print(f\"Setup {i+1}: {setup} → {len(names)} model names\")\n",
    "\n",
    "\n",
    "training_data_directories = [\n",
    "    \"data_eP\",\n",
    "]\n",
    "\n",
    "training_datasets = []\n",
    "for base_path, data_dir, is_supervised in zip(base_paths, training_data_directories, supervised):\n",
    "    training_data = load_training_dataset(f\"{base_path}{data_dir}/training_data.pkl\", supervised=is_supervised)\n",
    "    training_datasets.append(training_data)\n",
    "\n",
    "# Print the training dataset shapes with their paths\n",
    "print(\"Training datasets loaded:\")\n",
    "for base_path, data_dir, training_data in zip(base_paths, training_data_directories, training_datasets):\n",
    "    if training_data is not None:\n",
    "        print(f\"Path: {base_path}{data_dir}/training_data.pkl, Shape: {training_data.shape}\")\n",
    "    else:\n",
    "        print(f\"Path: {base_path}{data_dir}/training_data.pkl, Shape: Not Loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e57c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFSampler:\n",
    "    def __init__(self, path_NF_model, seed=None):\n",
    "        self.dim = 2\n",
    "        self.bounds = np.array([[0.02, 1.2],  # m\n",
    "                                [1, 10],    # BG\n",
    "                                [0.05, 3],  # BGq\n",
    "                                [0, 1.5],   # smearQsWidth\n",
    "                                [0.05, 1.50], # QsmuRatio\n",
    "                                [0.02, 1.2], # m_jimwlk\n",
    "                                [0.0001, 0.28] # Lambda_QCD_jimwlk\n",
    "                                ])\n",
    "\n",
    "        # Specify to use CPU, not GPU.\n",
    "        jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "        if seed is None:\n",
    "            seed = time.time_ns()\n",
    "        self.sample_key, self.init_key = jax.random.split(jax.random.PRNGKey(seed), 2)\n",
    "\n",
    "        # Load the normalizing flow and its hyperparameters\n",
    "        self.flow, self.hyperparams = load(path_NF_model, self.init_key)\n",
    "        self.dimension = self.hyperparams['dimension']\n",
    "\n",
    "    def _in_bounds(self, x):\n",
    "        lower = jnp.array(self.bounds[:, 0])\n",
    "        upper = jnp.array(self.bounds[:, 1])\n",
    "        return jnp.all((x >= lower) & (x <= upper), axis=1)\n",
    "\n",
    "    def sample(self, size=1):\n",
    "        accepted = []\n",
    "        while len(accepted) < size:\n",
    "            self.sample_key, pkey = jax.random.split(self.sample_key)\n",
    "            z = jax.random.normal(pkey, (size, self.dimension))\n",
    "            x, _ = self.flow(z)\n",
    "\n",
    "            mask = np.array(self._in_bounds(x))\n",
    "            x_np = np.array(x)\n",
    "            accepted.extend(x_np[mask])\n",
    "\n",
    "        return np.array(accepted[:size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corner_comparison_plot(training_data, model_samples, bounds, labels=None, bins=50, figsize=(12, 12),\n",
    "                                save_path=None,\n",
    "                                kl_1d=None, kl_2d=None):\n",
    "    \"\"\"\n",
    "    Create a corner plot comparing training_data and model_samples with fixed bounds.\n",
    "    \n",
    "    Parameters:\n",
    "    - training_data: (N, D) numpy array\n",
    "    - model_samples: (M, D) numpy array\n",
    "    - bounds: (D, 2) array of [min, max] for each dimension\n",
    "    - labels: list of length D with parameter names\n",
    "    - bins: number of histogram bins\n",
    "    - figsize: figure size\n",
    "    - save_path: path to save the plot (optional)\n",
    "    - kl_1d: optional array of shape (D,) with 1D KL divergences\n",
    "    - kl_2d: optional array of shape (D, D) with 2D KL divergences\n",
    "    \"\"\"\n",
    "    D = training_data.shape[1]\n",
    "    fig, axes = plt.subplots(D, D, figsize=figsize, sharex='col', sharey='row')\n",
    "\n",
    "    for i in range(D):\n",
    "        for j in range(D):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            if i == j:\n",
    "                # Diagonal: 1D histograms\n",
    "                ax = fig.add_subplot(D, D, i * D + j + 1)\n",
    "                ax.hist(training_data[:, i], bins=bins, density=True, alpha=0.6,\n",
    "                        color='tab:green', label='Training', range=bounds[i])\n",
    "                ax.hist(model_samples[:, i], bins=bins, density=True, alpha=0.6,\n",
    "                        color='tab:purple', label='NF', range=bounds[i])\n",
    "                ax.set_yticks([])\n",
    "                ax.set_yticklabels([])\n",
    "                if i != D-1:\n",
    "                    ax.set_xticks([])\n",
    "                    ax.set_xticklabels([])\n",
    "                if i == D-1:\n",
    "                    ax.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "\n",
    "                ax.set_xlim(bounds[i])\n",
    "                ax.set_ylim(0, None)\n",
    "\n",
    "                #if i == D-1:\n",
    "                #    ax.legend(fontsize=10)\n",
    "\n",
    "                if kl_1d is not None:\n",
    "                    ax.annotate(rf\"$D_{{KL}}={kl_1d[i]:.3f}$\", xy=(0.95, 0.95), \n",
    "                                xycoords='axes fraction',\n",
    "                                ha='right', va='top', fontsize=10)\n",
    "            elif j < i:\n",
    "                # Lower triangle: 2D histograms\n",
    "                ax.hist2d(training_data[:, j], training_data[:, i], bins=bins,\n",
    "                          range=[bounds[j], bounds[i]], cmap='Greens', alpha=0.4)\n",
    "                ax.hist2d(model_samples[:, j], model_samples[:, i], bins=bins,\n",
    "                          range=[bounds[j], bounds[i]], cmap='RdPu', alpha=0.4)\n",
    "                ax.set_xlim(bounds[j])\n",
    "                ax.set_ylim(bounds[i])\n",
    "\n",
    "                if kl_2d is not None and not np.isnan(kl_2d[i, j]):\n",
    "                    ax.annotate(rf\"$D_{{KL}}={kl_2d[i, j]:.3f}$\", xy=(0.95, 0.95), \n",
    "                                xycoords='axes fraction',\n",
    "                                ha='right', va='top', fontsize=10)\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "\n",
    "            # Labeling\n",
    "            if i == D - 1:\n",
    "                ax.set_xlabel(labels[j])\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(labels[i])\n",
    "\n",
    "            if i < j:\n",
    "                ax.axis('off')\n",
    "\n",
    "    # Remove space between subplots\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.subplots_adjust(left=0.07, right=0.95, top=0.95, bottom=0.07)\n",
    "\n",
    "    # Remove ticks and labels for the first and last plots\n",
    "    axes[0, 0].tick_params(axis='both', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    axes[D - 1, D - 1].tick_params(axis='both', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "\n",
    "    # Rotate x-axis tick labels\n",
    "    for ax in axes[-1]:\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=14)\n",
    "\n",
    "    # Rotate y-axis tick labels\n",
    "    for ax in axes[:, 0]:\n",
    "        ax.tick_params(axis='y', rotation=45, labelsize=14)\n",
    "\n",
    "    fig.legend(\n",
    "        labels=[\"Training\", \"NF\"], \n",
    "        loc=\"upper right\", \n",
    "        fontsize=12,\n",
    "        frameon=False,\n",
    "        bbox_to_anchor=(0.35, 0.95)\n",
    "    )\n",
    "\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "    del fig, axes\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e858b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_1d_kl(p_samples, q_samples, bounds, bins=100, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Computes the KL divergence for each 1D marginal (diagonal of corner plot).\n",
    "    Arguments:\n",
    "        p_samples, q_samples: Arrays of shape (n_samples, n_dims)\n",
    "        bounds: np.ndarray of shape (n_dims, 2) specifying min and max for each dimension\n",
    "    Returns:\n",
    "        kls: List of KL divergences for each dimension\n",
    "        mean_kl: Mean KL divergence across all dimensions\n",
    "    \"\"\"\n",
    "    kls = []\n",
    "    dim = p_samples.shape[1]\n",
    "    for i in range(dim):\n",
    "        range_i = bounds[i]\n",
    "        p_hist, bin_edges = np.histogram(p_samples[:, i], bins=bins, range=range_i, density=True)\n",
    "        q_hist, _ = np.histogram(q_samples[:, i], bins=bin_edges, density=True)\n",
    "\n",
    "        # Avoid log(0)\n",
    "        p_hist += epsilon\n",
    "        q_hist += epsilon\n",
    "\n",
    "        kl = entropy(p_hist, q_hist)\n",
    "        kls.append(kl)\n",
    "    return kls, np.mean(kls)\n",
    "\n",
    "\n",
    "def compute_2d_kl_lower(p_samples, q_samples, bounds, bins=40, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Computes KL divergence for each 2D correlation (lower triangle of corner plot).\n",
    "    Arguments:\n",
    "        p_samples, q_samples: Arrays of shape (n_samples, n_dims)\n",
    "        bounds: np.ndarray of shape (n_dims, 2) specifying min and max for each dimension\n",
    "    Returns:\n",
    "        kls_2d: List of tuples ((i, j), kl) for each lower-triangle pair\n",
    "        avg_kl_2d: Mean KL divergence across all pairs\n",
    "    \"\"\"\n",
    "    kls_2d = []\n",
    "    dim = p_samples.shape[1]\n",
    "    for i in range(dim):\n",
    "        for j in range(i):\n",
    "            range_2d = [bounds[i], bounds[j]]\n",
    "            p_hist, xedges, yedges = np.histogram2d(\n",
    "                p_samples[:, i], p_samples[:, j], bins=bins, range=range_2d, density=True\n",
    "            )\n",
    "            q_hist, _, _ = np.histogram2d(\n",
    "                q_samples[:, i], q_samples[:, j], bins=[xedges, yedges], density=True\n",
    "            )\n",
    "\n",
    "            # Flatten and regularize\n",
    "            p_flat = p_hist.flatten() + epsilon\n",
    "            q_flat = q_hist.flatten() + epsilon\n",
    "\n",
    "            kl_2d = entropy(p_flat, q_flat)\n",
    "            kls_2d.append(((i, j), kl_2d))\n",
    "    \n",
    "    avg_kl_2d = np.mean([k[1] for k in kls_2d]) if kls_2d else 0.0\n",
    "    return kls_2d, avg_kl_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b37c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_labels = [\n",
    "    r\"$m\\;[\\mathrm{GeV}]$\", \n",
    "    r\"$B_G\\;[\\mathrm{GeV}^{-2}]$\", \n",
    "    r\"$B_{q}\\;[\\mathrm{GeV}^{-2}]$\", \n",
    "    r\"$\\sigma$\", \n",
    "    r\"$Q_s/(g^2\\mu)$\", \n",
    "    r\"$m_{\\mathrm{JIMWLK}}\\;[\\mathrm{GeV}]$\", \n",
    "    r\"$\\Lambda_{\\mathrm{QCD}}\\;[\\mathrm{GeV}]$\"\n",
    "]\n",
    "\n",
    "best_models = []\n",
    "for model_setup, training_data in zip(model_names, training_datasets):\n",
    "    print(f\"Processing model setup with {len(model_setup)} models...\")\n",
    "    model_setup_found = []\n",
    "    avg_kl_values = []\n",
    "    for model_name in model_setup:\n",
    "        # If model does not exist, skip\n",
    "        try:\n",
    "            print(f\"Evaluating model: {model_name}\")\n",
    "            sampler = NFSampler(model_name)\n",
    "            #model_samples = np.array(sampler.sample(size=training_data.shape[0]))\n",
    "            model_samples = jax.device_get(sampler.sample(size=training_data.shape[0]))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Model file not found: {model_name}\")\n",
    "            continue\n",
    "\n",
    "        # Compute KL divergences\n",
    "        kl_1d, mean_kl_1d = compute_1d_kl(\n",
    "            p_samples=training_data,\n",
    "            q_samples=model_samples,\n",
    "            bounds=sampler.bounds,\n",
    "            bins=75\n",
    "        )\n",
    "        kl_2d, mean_kl_2d = compute_2d_kl_lower(\n",
    "            p_samples=training_data,\n",
    "            q_samples=model_samples,\n",
    "            bounds=sampler.bounds,\n",
    "            bins=75\n",
    "        )\n",
    "\n",
    "        # Overall mean KL divergence\n",
    "        overall_mean_kl = (mean_kl_1d + mean_kl_2d) / 2.\n",
    "        avg_kl_values.append(overall_mean_kl)\n",
    "        model_setup_found.append(model_name)\n",
    "\n",
    "        kl_2d_dict = dict(kl_2d)\n",
    "        make_corner_comparison_plot(\n",
    "            training_data=training_data,\n",
    "            model_samples=model_samples,\n",
    "            bounds=sampler.bounds,\n",
    "            labels=param_labels,\n",
    "            bins=75,\n",
    "            save_path=f\"{model_name}_corner_plot.png\",\n",
    "            kl_1d=kl_1d,\n",
    "            kl_2d=kl_2d_dict\n",
    "        )\n",
    "\n",
    "        # Explicitly delete all objects to free memory\n",
    "        del sampler, model_samples, kl_1d, kl_2d, kl_2d_dict\n",
    "        gc.collect()\n",
    "        print_memory_usage()\n",
    "\n",
    "    # Identify the model with the lowest average KL divergence\n",
    "    best_model_index = np.argmin(avg_kl_values)\n",
    "    best_model_name = model_setup_found[best_model_index]\n",
    "    best_models.append(f\"Best model: {best_model_name} with KL divergence: {avg_kl_values[best_model_index]:.4f}\")\n",
    "\n",
    "# Print the best models for each setup\n",
    "print(\"\\nBest models for each setup:\")\n",
    "for i, best_model in enumerate(best_models):\n",
    "    print(f\"Setup {i+1}: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026efa84",
   "metadata": {},
   "source": [
    "## Plot for the paper to compare the NF flow model to the training distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c86944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def make_corner_comparison_plot_paper(training_data, model_samples, bounds, labels=None, bins=50, figsize=(12, 12),\n",
    "                                save_path=None,\n",
    "                                kl_1d=None, kl_2d=None, mean_kl=None):\n",
    "    \"\"\"\n",
    "    Create a corner plot comparing training_data and model_samples with fixed bounds.\n",
    "    \n",
    "    Parameters:\n",
    "    - training_data: (N, D) numpy array\n",
    "    - model_samples: (M, D) numpy array\n",
    "    - bounds: (D, 2) array of [min, max] for each dimension\n",
    "    - labels: list of length D with parameter names\n",
    "    - bins: number of histogram bins\n",
    "    - figsize: figure size\n",
    "    - save_path: path to save the plot (optional)\n",
    "    - kl_1d: optional array of shape (D,) with 1D KL divergences\n",
    "    - kl_2d: optional array of shape (D, D) with 2D KL divergences\n",
    "    - mean_kl: overall mean KL divergence (optional)\n",
    "    \"\"\"\n",
    "    D = training_data.shape[1]\n",
    "    fig, axes = plt.subplots(D, D, figsize=figsize, sharex='col', sharey='row')\n",
    "\n",
    "    for i in range(D):\n",
    "        for j in range(D):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            if i == j:\n",
    "                # Diagonal: 1D histograms\n",
    "                ax = fig.add_subplot(D, D, i * D + j + 1)\n",
    "                ax.hist(training_data[:, i], bins=bins, density=True, alpha=0.5,\n",
    "                        color='tab:green', label='Training', range=bounds[i])\n",
    "                ax.hist(model_samples[:, i], bins=bins, density=True,\n",
    "                        color='tab:orange', histtype='step', linewidth=2, \n",
    "                        label='NF', range=bounds[i], linestyle=':')\n",
    "                ax.set_yticks([])\n",
    "                ax.set_yticklabels([])\n",
    "                if i != D-1:\n",
    "                    ax.set_xticks([])\n",
    "                    ax.set_xticklabels([])\n",
    "                if i == D-1:\n",
    "                    ax.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "\n",
    "                ax.set_xlim(bounds[i])\n",
    "                ax.set_ylim(0, None)\n",
    "\n",
    "                if kl_1d is not None:\n",
    "                    ax.annotate(rf\"$D_{{KL}}={kl_1d[i]:.3f}$\", xy=(0.95, 0.95), \n",
    "                                xycoords='axes fraction',\n",
    "                                ha='right', va='top', fontsize=10)\n",
    "            elif j < i:\n",
    "                # Lower triangle: 2D histograms\n",
    "                ax.hist2d(training_data[:, j], training_data[:, i], bins=bins,\n",
    "                          range=[bounds[j], bounds[i]], cmap='Greens', alpha=0.5)\n",
    "\n",
    "                def get_contour_levels(Z, levels):\n",
    "                    Z_flat = Z.flatten()\n",
    "                    Z_sorted = np.sort(Z_flat)[::-1]\n",
    "                    cumsum = np.cumsum(Z_sorted)\n",
    "                    cumsum /= cumsum[-1]  # normalize to [0,1]\n",
    "                    levels_out = [Z_sorted[np.searchsorted(cumsum, level)] for level in levels]\n",
    "                    return sorted(levels_out)\n",
    "\n",
    "                x = np.linspace(bounds[j][0], bounds[j][1], bins)\n",
    "                y = np.linspace(bounds[i][0], bounds[i][1], bins)\n",
    "                X, Y = np.meshgrid(x, y)\n",
    "                # Add contour for training_data (solid)\n",
    "                try:\n",
    "                    data_train = np.vstack([training_data[:, j], training_data[:, i]])\n",
    "                    kde_train = gaussian_kde(data_train)\n",
    "                    Z_train = kde_train(np.vstack([X.ravel(), Y.ravel()])).reshape(X.shape)\n",
    "                    contour_levels = get_contour_levels(Z_train, levels=[0.685, 0.955, 0.997])\n",
    "                    ax.contour(X, Y, Z_train, levels=contour_levels, colors='green', linewidths=1.2)\n",
    "                except Exception as e:\n",
    "                    print(f\"Contour failed for training data at ({i},{j}): {e}\")\n",
    "                \n",
    "                # Add contour for model_samples (dashed)\n",
    "                try:\n",
    "                    data_model = np.vstack([model_samples[:, j], model_samples[:, i]])\n",
    "                    kde_model = gaussian_kde(data_model)\n",
    "                    Z_model = kde_model(np.vstack([X.ravel(), Y.ravel()])).reshape(X.shape)\n",
    "                    contour_levels_model = get_contour_levels(Z_model, levels=[0.685, 0.955, 0.997])\n",
    "                    ax.contour(X, Y, Z_model, levels=contour_levels_model, colors='orange',\n",
    "                            linewidths=1.5, linestyles='dotted')\n",
    "                except Exception as e:\n",
    "                    print(f\"Contour failed for model data at ({i},{j}): {e}\")\n",
    "\n",
    "                ax.set_xlim(bounds[j])\n",
    "                ax.set_ylim(bounds[i])\n",
    "\n",
    "                if kl_2d is not None and not np.isnan(kl_2d[i, j]):\n",
    "                    ax.annotate(rf\"$D_{{KL}}={kl_2d[i, j]:.3f}$\", xy=(0.95, 0.95), \n",
    "                                xycoords='axes fraction',\n",
    "                                ha='right', va='top', fontsize=10)\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "\n",
    "            # Labeling\n",
    "            if i == D - 1:\n",
    "                ax.set_xlabel(labels[j], fontsize=14)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(labels[i], fontsize=14)\n",
    "\n",
    "            if i < j:\n",
    "                ax.axis('off')\n",
    "\n",
    "    # Remove space between subplots\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.subplots_adjust(left=0.07, right=0.95, top=0.95, bottom=0.07)\n",
    "\n",
    "    # Remove ticks and labels for the first and last plots\n",
    "    axes[0, 0].tick_params(axis='both', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    axes[D - 1, D - 1].tick_params(axis='both', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "\n",
    "    # Rotate x-axis tick labels\n",
    "    for ax in axes[-1]:\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=14)\n",
    "\n",
    "    # Rotate y-axis tick labels\n",
    "    for ax in axes[:, 0]:\n",
    "        ax.tick_params(axis='y', rotation=45, labelsize=14)\n",
    "\n",
    "    fig.legend(\n",
    "        labels=[\"Training\", \"NF\"], \n",
    "        loc=\"upper right\", \n",
    "        fontsize=12,\n",
    "        frameon=False,\n",
    "        bbox_to_anchor=(0.35, 0.95)\n",
    "    )\n",
    "\n",
    "    if mean_kl is not None:\n",
    "        # annotate mean KL divergence with bbox_to_anchor annotate\n",
    "        fig.text(0.33, 0.87, rf\"$\\langle D_{{KL}} \\rangle = {mean_kl:.3f}$\", \n",
    "                 ha='right', va='bottom', fontsize=12)\n",
    "\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "    del fig, axes\n",
    "    gc.collect()\n",
    "\n",
    "param_labels = [\n",
    "    r\"$m\\;[\\mathrm{GeV}]$\", \n",
    "    r\"$B_G\\;[\\mathrm{GeV}^{-2}]$\", \n",
    "    r\"$B_{q}\\;[\\mathrm{GeV}^{-2}]$\", \n",
    "    r\"$\\sigma$\", \n",
    "    r\"$Q_s/(g^2\\mu)$\", \n",
    "    r\"$m_{\\mathrm{JIMWLK}}\\;[\\mathrm{GeV}]$\", \n",
    "    r\"$\\Lambda_{\\mathrm{QCD}}\\;[\\mathrm{GeV}]$\"\n",
    "]\n",
    "\n",
    "base_paths = [\n",
    "    \"./eP/\",\n",
    "]\n",
    "\n",
    "corresponding_model_dir_names = [\n",
    "    \"models_eP/\",\n",
    "]\n",
    "\n",
    "filename_prefixes = [\n",
    "    \"eP_nf_bs\",\n",
    "]\n",
    "\n",
    "supervised = [True, True]\n",
    "\n",
    "model_names = [\n",
    "    [\"./CLUSTER/eP/models_eP/eP_nf_bs5000_L6_lr1e-03.pkl\"],\n",
    "]\n",
    "\n",
    "# Check results\n",
    "print(f\"{len(model_names)} total setups.\")\n",
    "for i, (setup, names) in enumerate(zip(base_paths, model_names)):\n",
    "    print(f\"Setup {i+1}: {setup} → {len(names)} model names\")\n",
    "\n",
    "\n",
    "training_data_directories = [\n",
    "    \"data_eP\",\n",
    "]\n",
    "\n",
    "training_datasets = []\n",
    "for base_path, data_dir, is_supervised in zip(base_paths, training_data_directories, supervised):\n",
    "    training_data = load_training_dataset(f\"{base_path}{data_dir}/training_data.pkl\", supervised=is_supervised)\n",
    "    training_datasets.append(training_data)\n",
    "\n",
    "# Print the training dataset shapes with their paths\n",
    "print(\"Training datasets loaded:\")\n",
    "for base_path, data_dir, training_data in zip(base_paths, training_data_directories, training_datasets):\n",
    "    if training_data is not None:\n",
    "        print(f\"Path: {base_path}{data_dir}/training_data.pkl, Shape: {training_data.shape}\")\n",
    "    else:\n",
    "        print(f\"Path: {base_path}{data_dir}/training_data.pkl, Shape: Not Loaded\")\n",
    "\n",
    "i = 0\n",
    "for model_setup, training_data in zip(model_names, training_datasets):\n",
    "    print(f\"Processing model setup with {len(model_setup)} models...\")\n",
    "    for model_name in model_setup:\n",
    "        # If model does not exist, skip\n",
    "        try:\n",
    "            print(f\"Evaluating model: {model_name}\")\n",
    "            sampler = NFSampler(model_name)\n",
    "            model_samples = jax.device_get(sampler.sample(size=training_data.shape[0]))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Model file not found: {model_name}\")\n",
    "            continue\n",
    "\n",
    "        # Compute KL divergences\n",
    "        kl_1d, mean_kl_1d = compute_1d_kl(\n",
    "            p_samples=training_data,\n",
    "            q_samples=model_samples,\n",
    "            bounds=sampler.bounds,\n",
    "            bins=75\n",
    "        )\n",
    "        kl_2d, mean_kl_2d = compute_2d_kl_lower(\n",
    "            p_samples=training_data,\n",
    "            q_samples=model_samples,\n",
    "            bounds=sampler.bounds,\n",
    "            bins=75\n",
    "        )\n",
    "\n",
    "        kl_2d_dict = dict(kl_2d)\n",
    "        # Overall mean KL divergence\n",
    "        overall_mean_kl = (mean_kl_1d + mean_kl_2d) / 2.\n",
    "        make_corner_comparison_plot_paper(\n",
    "            training_data=training_data,\n",
    "            model_samples=model_samples,\n",
    "            bounds=sampler.bounds,\n",
    "            labels=param_labels,\n",
    "            bins=75,\n",
    "            save_path=f\"./NF_trained_corner_plot_{training_data_directories[i]}.pdf\",\n",
    "            kl_1d=None,\n",
    "            kl_2d=None,\n",
    "            mean_kl=overall_mean_kl\n",
    "        )\n",
    "\n",
    "        # Explicitly delete all objects to free memory\n",
    "        del sampler, model_samples, kl_1d, kl_2d, kl_2d_dict\n",
    "        gc.collect()\n",
    "        print_memory_usage()\n",
    "        i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
